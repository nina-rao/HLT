{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "# Nina Rao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a hierarchical database of nouns, verbs, adjectives, and adverbs that includes glosses, synsets, use examples, and relations to other words. It is used to analyze semantic relations between words. Glosses are the definitions of words. Synsets are sets of synonyms and are connected to other synsets through these semantic relations. These include hypernym, hyponym, meronym, holonym, and troponym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('blood.n.01'),\n",
       " Synset('blood.n.02'),\n",
       " Synset('rake.n.01'),\n",
       " Synset('lineage.n.01'),\n",
       " Synset('blood.n.05'),\n",
       " Synset('blood.v.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOUN : dream\n",
    "# get all synsets of 'dream'\n",
    "wn.synsets('blood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fluid (red in vertebrates) that is pumped through the body by the heart and contains plasma, blood cells, and platelets'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definition \n",
    "wn.synset('blood.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blood carries oxygen and nutrients to the tissues and carries away waste products',\n",
       " 'the ancients believed that blood was the seat of the emotions']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract examples\n",
    "wn.synset('blood.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('blood.n.01.blood')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract lemmas\n",
    "wn.synset('blood.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('liquid_body_substance.n.01')\n",
      "Synset('body_substance.n.01')\n",
      "Synset('substance.n.01')\n",
      "Synset('matter.n.03')\n",
      "Synset('physical_entity.n.01')\n",
      "Synset('entity.n.01')\n"
     ]
    }
   ],
   "source": [
    "# traversing hierarchy\n",
    "blood = wn.synset('blood.n.01')\n",
    "hyp = blood.hypernyms()[0]\n",
    "top = wn.synset('entity.n.01')\n",
    "while hyp:\n",
    "    print(hyp)\n",
    "    if hyp == top:\n",
    "        break\n",
    "    if hyp.hypernyms():\n",
    "        hyp = hyp.hypernyms()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet organizes nouns by their relations to other nouns hierarchically. These nouns are linked by varying degrees of broadness upward and specificity downward. As you move up, you could consider the word to be a generalization of the former word. We stop once we reach entity because that is the most generalized form of the word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('liquid_body_substance.n.01')]\n",
      "[Synset('arterial_blood.n.01'), Synset('blood_clot.n.01'), Synset('blood_group.n.01'), Synset('bloodstream.n.01'), Synset('cord_blood.n.01'), Synset('gore.n.02'), Synset('lifeblood.n.01'), Synset('menorrhea.n.01'), Synset('venous_blood.n.01'), Synset('whole_blood.n.01')]\n",
      "[Synset('blood_cell.n.01')]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "blood_lemma = wn.lemma('blood.n.01.blood')\n",
    "\n",
    "# hypernyms\n",
    "print(blood.hypernyms())\n",
    "\n",
    "# hyponyms\n",
    "print(blood.hyponyms())\n",
    "\n",
    "# meronyms\n",
    "print(blood.part_meronyms())\n",
    "\n",
    "# holonyms\n",
    "print(blood.part_holonyms())\n",
    "\n",
    "# antonyms\n",
    "print(blood_lemma.antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('play.n.01'),\n",
       " Synset('play.n.02'),\n",
       " Synset('play.n.03'),\n",
       " Synset('maneuver.n.03'),\n",
       " Synset('play.n.05'),\n",
       " Synset('play.n.06'),\n",
       " Synset('bid.n.02'),\n",
       " Synset('play.n.08'),\n",
       " Synset('playing_period.n.01'),\n",
       " Synset('free_rein.n.01'),\n",
       " Synset('shimmer.n.01'),\n",
       " Synset('fun.n.02'),\n",
       " Synset('looseness.n.05'),\n",
       " Synset('play.n.14'),\n",
       " Synset('turn.n.03'),\n",
       " Synset('gambling.n.01'),\n",
       " Synset('play.n.17'),\n",
       " Synset('play.v.01'),\n",
       " Synset('play.v.02'),\n",
       " Synset('play.v.03'),\n",
       " Synset('act.v.03'),\n",
       " Synset('play.v.05'),\n",
       " Synset('play.v.06'),\n",
       " Synset('play.v.07'),\n",
       " Synset('act.v.05'),\n",
       " Synset('play.v.09'),\n",
       " Synset('play.v.10'),\n",
       " Synset('play.v.11'),\n",
       " Synset('play.v.12'),\n",
       " Synset('play.v.13'),\n",
       " Synset('play.v.14'),\n",
       " Synset('play.v.15'),\n",
       " Synset('play.v.16'),\n",
       " Synset('play.v.17'),\n",
       " Synset('play.v.18'),\n",
       " Synset('toy.v.02'),\n",
       " Synset('play.v.20'),\n",
       " Synset('dally.v.04'),\n",
       " Synset('play.v.22'),\n",
       " Synset('dally.v.01'),\n",
       " Synset('play.v.24'),\n",
       " Synset('act.v.10'),\n",
       " Synset('play.v.26'),\n",
       " Synset('bring.v.03'),\n",
       " Synset('play.v.28'),\n",
       " Synset('play.v.29'),\n",
       " Synset('bet.v.02'),\n",
       " Synset('play.v.31'),\n",
       " Synset('play.v.32'),\n",
       " Synset('play.v.33'),\n",
       " Synset('meet.v.10'),\n",
       " Synset('play.v.35')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VERB : play\n",
    "# get all synsets of play\n",
    "wn.synsets('play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play on an instrument'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definition \n",
    "wn.synset('play.v.03').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The band played all night long']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract examples\n",
    "wn.synset('play.v.03').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('play.v.03.play')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract lemmas\n",
    "wn.synset('play.v.03').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('perform.v.03'), Synset('re-create.v.01'), Synset('make.v.03')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play = wn.synset('play.v.03')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(play.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet also organizes verbs by their hierarchical relation to other verbs. For the verbs, we used the closure method defined in nltk in order to display the hierarchy. I noticed that play is a word with many meanings and has a generalized use case as well, so its closure is composed more so of synonyms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Morphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'godly'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.morphy('godliest', wn.ADJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('desert.n.01'), Synset('abandon.v.05'), Synset('defect.v.01'), Synset('desert.v.03')]\n",
      "arid land with little or no vegetation \n",
      "\n",
      "[Synset('tundra.n.01')]\n",
      "a vast treeless plain in the Arctic regions where the subsoil is permanently frozen\n"
     ]
    }
   ],
   "source": [
    "# desert vs. tundra\n",
    "# a tundra gets very minimal precipitation annually, technically classifying\n",
    "# it as a desert. However deserts\n",
    "print(wn.synsets('desert'))\n",
    "print(wn.synset('desert.n.01').definition(), \"\\n\")\n",
    "desert = wn.synset('desert.n.01')\n",
    "\n",
    "print(wn.synsets('tundra'))\n",
    "print(wn.synset('tundra.n.01').definition())\n",
    "tundra = wn.synset('tundra.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wu-Palmer similarity metric\n",
    "wn.wup_similarity(desert, tundra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('desert.n.01')\n",
      "Synset('tundra.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Lesk algorithm\n",
    "from nltk.wsd import lesk\n",
    "# the Arctic has tundra and desert like qualities\n",
    "sent1 = ['The', 'Artic', 'is', 'a', 'rugged', 'biome', 'with', 'little', 'biodiversity', ',', 'the', 'flora', 'and', 'fauna', 'must', 'endure', 'brutal', 'cold', 'and', 'winds', '.']\n",
    "print(lesk(sent1, 'desert'))\n",
    "print(lesk(sent1, 'tundra'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that the Wu-Palmer similarity metric gave these two words an index of 0.43 out of 1. It was able to recognize the similarities of these words by their definitions (both being biome types having little precipitation, little to no vegetation, and extreme temperatures) but also see major differences, which I believe is reflected in the score. Meanwhile, the Lesk algorithm uses word overlap count by looking at context and comparing to glosses. It correctly placed both definitions, however both these words synsets were very small so that should be taken into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiWordNet is an extension of WordNet and deals with the emotional connotations of words. It is a type of sentiment analyzer that assigns scores to words. There are 3 types of scores: positive, negative, and objective. This can be used to identify sentiment in text and has applications in detecting hate speech on social media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euphoria: [Synset('euphoria.n.01')]\n",
      "a feeling of great (usually exaggerated) elation \n",
      "\n",
      "<euphoria.n.01: PosScore=0.125 NegScore=0.5>\n",
      "Positive score =  0.125\n",
      "Negative score =  0.5\n",
      "Objective score =  0.375\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "print(\"euphoria:\", wn.synsets('euphoria'))\n",
    "euphoric = swn.senti_synset('euphoria.n.01')\n",
    "print(wn.synset('euphoria.n.01').definition(), \"\\n\")\n",
    "print(euphoric)\n",
    "print(\"Positive score = \", euphoric.pos_score())\n",
    "print(\"Negative score = \", euphoric.neg_score())\n",
    "print(\"Objective score = \", euphoric.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eating.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive score =  0.0\n",
      "Negative score =  0.0\n",
      "Objective score =  1.0\n",
      "<piquant.s.01: PosScore=0.375 NegScore=0.5>\n",
      "Positive score =  0.375\n",
      "Negative score =  0.5\n",
      "Objective score =  0.125\n",
      "<food.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive score =  0.0\n",
      "Negative score =  0.0\n",
      "Objective score =  1.0\n",
      "<injury.n.01: PosScore=0.0 NegScore=0.625>\n",
      "Positive score =  0.0\n",
      "Negative score =  0.625\n",
      "Objective score =  0.375\n",
      "<sol.n.03: PosScore=0.0 NegScore=0.0>\n",
      "Positive score =  0.0\n",
      "Negative score =  0.0\n",
      "Objective score =  1.0\n",
      "<good.n.01: PosScore=0.5 NegScore=0.0>\n",
      "Positive score =  0.5\n",
      "Negative score =  0.0\n",
      "Objective score =  0.5\n"
     ]
    }
   ],
   "source": [
    "sent = \"Eating spicy food hurts so good\"\n",
    "words = sent.split()\n",
    "for w in words:\n",
    "    word = list(swn.senti_synsets(w))[0]\n",
    "    print(word)\n",
    "    print(\"Positive score = \", word.pos_score())\n",
    "    print(\"Negative score = \", word.neg_score())\n",
    "    print(\"Objective score = \", word.obj_score())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intentionally picked a contradictory sentence with language that a native English speaker would immediately understand the sentiment of, but a text processor may not grasp that \"hurts so good\" means that this is a type of pain that one enjoys. SentiWordNet gave spicy a high negative score which is interesting as spicy is a subjective term for many. Overall, SentiWordNet is a useful tool for assessing sentiment in a large corpus of text but for sentences like these with ambiguity and idioms, it may not be entirely accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collocation is 2+ words that appear so often together that they can be considered a unique phrase that will not hold the same meaning if a word is replaced. Words in a collocation are typically recognized because they are combined more than probabilistically expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Inaugural Address Corpus>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fellow - Citizens of the Senate and of the House o'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mutual information\n",
    "text = ' '.join(text4.tokens)\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Fellow Citizens) =  0.0004616805170821791\n",
      "p(Fellow) =  0.0110803324099723\n",
      "p(Citizens) =  0.003231763619575254\n",
      "pmi =  3.688500104829567\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "vocab = len(set(text6))\n",
    "fc = text.count('Fellow Citizens')/vocab\n",
    "print(\"p(Fellow Citizens) = \", fc)\n",
    "f = text.count('Fellow')/vocab\n",
    "print(\"p(Fellow) = \", f)\n",
    "c = text.count('Citizens')/vocab\n",
    "print(\"p(Citizens) = \", c)\n",
    "pmi = math.log2(fc / (f * c))\n",
    "print(\"pmi = \", pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that in text4 of nltk, 'Fellow Citizens' has a relatively mutual information score. Mutual information is calculated by the log of the probability of P(x,y)/P(x) * P(y) where P is the probability of the text appearing in the vocabulary of the corpus. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
