{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First, we do webscraping to scrape poems off of a poet's webpage."
      ],
      "metadata": {
        "id": "uHMO_zMQC1iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and downloads\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "uamgUTzr26UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwXaN7rX8FwO"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "from urllib.request import Request\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# function to start scraping at a poet's profile\n",
        "def urlgen():\n",
        "  # Try different URLs for different responses! But be warned, web scraping takes a few minutes\n",
        "  #https://mypoeticside.com/poets/oscar-wilde-poems\n",
        "  #https://mypoeticside.com/poets/william-shakespeare-poems\n",
        "  #https://mypoeticside.com/poets/maya-angelou-poems\n",
        "  #https://mypoeticside.com/poets/pablo-neruda-poems\n",
        "  url = 'https://mypoeticside.com/poets/emily-dickinson-poems'\n",
        "  return url\n",
        "\n",
        "# function to scrape text off each page and store in file\n",
        "def webscrape(link):\n",
        "    with open(\"chatbot_data.txt\", \"a+\", encoding='utf-8') as f:\n",
        "        req = Request(\n",
        "            url=link,\n",
        "            headers={'User-Agent': 'Mozilla/5.0'}\n",
        "        )\n",
        "        try:\n",
        "          html = request.urlopen(req).read().decode('utf8')\n",
        "          soup = BeautifulSoup(html, features='html.parser')\n",
        "          poem_list = soup.find(class_=\"list-poems\")\n",
        "          if poem_list is not None:\n",
        "            links = poem_list.findAll('a')\n",
        "            results = [\"https:\"+link.get('href') for link in links]\n",
        "            for page in results:\n",
        "              try:\n",
        "                r = request.urlopen(page).read().decode('utf8')\n",
        "                soup = BeautifulSoup(r, features='html.parser')\n",
        "                poem = soup.find(class_='poem-entry')\n",
        "                if poem is not None:\n",
        "                  text = poem.find('p').getText()\n",
        "                  text_chunks = [chunk for chunk in text.splitlines() if not re.match(r'^\\s*$', chunk)]\n",
        "                  for chunk in text_chunks:\n",
        "                    f.write(chunk)\n",
        "                    f.write('\\n')\n",
        "              except:\n",
        "                print(\"An error occurred\")\n",
        "        except:\n",
        "          print(\"An error occurred\")\n",
        "\n",
        "# function to tokenize and find top terms\n",
        "def preprocess(file_name):\n",
        "  # tokenize\n",
        "  text = open(file_name).read()\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  new_tokens = [t for t in tokens if not t in stop_words and len(t) > 3]\n",
        "\n",
        "  # lemmatize\n",
        "  wnl = WordNetLemmatizer()\n",
        "  lemmas = set(wnl.lemmatize(t) for t in new_tokens)\n",
        "  list_lemmas = list(lemmas)\n",
        "\n",
        "  # part of speech tagging\n",
        "  pos_tags = nltk.pos_tag(list_lemmas)\n",
        "  \n",
        "  # isolate nouns\n",
        "  nouns = [n for (n, p) in pos_tags if p.startswith(\"N\")]\n",
        "\n",
        "  # find most common nouns\n",
        "  noun_dict = {nouns[i]: new_tokens.count(nouns[i]) for i in range(len(nouns))}\n",
        "  sort_nouns = sorted(noun_dict.items(), reverse=True, key=lambda x: x[1])\n",
        "  top = sort_nouns[0:50]\n",
        "  top_list = [x[0] for x in top]\n",
        "  t_file = open('terms.txt', 'a+')\n",
        "  for t in top_list:\n",
        "    t_file.write(t)\n",
        "    t_file.write('\\n')\n",
        "\n",
        "\n",
        "# start\n",
        "url_origin = urlgen()\n",
        "print(url_origin)\n",
        "\n",
        "webscrape(url_origin)\n",
        "\n",
        "f_name = 'chatbot_data.txt'\n",
        "preprocess(f_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the preprocess function determines the top 50 terms present in the text. These are the terms we will use to construct the Knowledge Base."
      ],
      "metadata": {
        "id": "0j5kIqut4-Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge Base construction\n",
        "# get the top terms\n",
        "f = open('terms.txt', 'r')\n",
        "text = f.read()\n",
        "terms = text.split()\n",
        "with open('terms.pkl', 'wb') as f:\n",
        "  pickle.dump(terms, f)\n",
        "#print(terms)\n",
        "\n",
        "# get the scraped poem stanzas\n",
        "f = open('chatbot_data.txt', 'r')\n",
        "stanzas = f.readlines()\n",
        "with open('stanzas.pkl', 'wb') as f:\n",
        "  pickle.dump(stanzas, f)\n",
        "#print(stanzas[:20])\n",
        "\n",
        "# construct it\n",
        "kb = {}\n",
        "\n",
        "for term in terms:\n",
        "  for stanza in stanzas:\n",
        "    if term in stanza:\n",
        "      if term not in kb:\n",
        "        kb[term] = [stanza]\n",
        "      else:\n",
        "        kb[term].append(stanza)\n",
        "\n",
        "# for key,value in kb.items():\n",
        "# \tprint(key, ':', value)\n",
        " \n",
        "# pickle it\n",
        "with open('knowledge_base.pkl', 'wb') as f:\n",
        "  pickle.dump(kb, f)"
      ],
      "metadata": {
        "id": "HxRlcuoP6HkF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the knowledge base, we can begin the code that will run the chatbot."
      ],
      "metadata": {
        "id": "hqayHF-VDO1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "JXMbyR5PWtQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import string\n",
        "import random\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def is_positive(sent) -> bool:\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  return sia.polarity_scores(sent)[\"compound\"] > 0\n",
        "\n",
        "\n",
        "def generate_reply(input, terms, stanzas, kb):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # if there is an exact match with one of the knowledge base keys pick from the knowledge base\n",
        "  for t in terms:\n",
        "    if t in input:\n",
        "      sentences = kb[t]\n",
        "      sentences.append(input)\n",
        "      # convert text into numerical vector\n",
        "      vectors = vectorizer.fit_transform(sentences)\n",
        "      # compute cosine similarity\n",
        "      cs = cosine_similarity(vectors[-1], vectors)\n",
        "      best_index = cs[0].argsort()[-2]\n",
        "      another = cs[0].argsort()[-4]\n",
        "      sentences.remove(input)\n",
        "      if sentences[best_index] == sentences[another]:\n",
        "        return sentences[best_index]\n",
        "      else:\n",
        "        return sentences[best_index] + sentences[another]\n",
        "  \n",
        "  # otherwise pick from all stanzas\n",
        "  stanzas.append(input)\n",
        "  vectors = vectorizer.fit_transform(stanzas)\n",
        "  # compute cosine similarity\n",
        "  cs = cosine_similarity(vectors[-1], vectors)\n",
        "  best_index = cs[0].argsort()[-2]\n",
        "  another = cs[0].argsort()[-4]\n",
        "  stanzas.remove(input)\n",
        "  if stanzas[best_index] == stanzas[another]:\n",
        "    return stanzas[best_index]\n",
        "  else:\n",
        "    return stanzas[best_index] + stanzas[another]\n",
        "\n",
        "def run_chatbot():\n",
        "  # load in pickles\n",
        "  terms = pickle.load(open('terms.pkl', 'rb'))\n",
        "  stanzas = pickle.load(open('stanzas.pkl', 'rb'))\n",
        "  kb = pickle.load(open('knowledge_base.pkl', 'rb'))\n",
        "\n",
        "  info_phrases =[\"I think\", \"I feel\", \"I like\", \"I dislike\", \"I love\", \"I hate\", \"I am\", \"I'm\", \"I want\", \"I don't want\", \"I have\", \"I don't have\", \"I know\", \"I don't know\"]\n",
        "  # initial message\n",
        "  name = input(\"ChatPoet: Good day! Welcome to ChatPoet, poetry is a literary art form focused on the aesthetics of language.\\n Feeling ruminative? Have a conversation with me! Before we begin, what is your name?\\n\")\n",
        "\n",
        "  # load up user model or create one if not\n",
        "  if os.path.exists('user_model.pkl'):\n",
        "    user_model = pickle.load(open('user_model.pkl', 'rb'))\n",
        "  else:\n",
        "    user_model = {}\n",
        "  \n",
        "  # check for returning user\n",
        "  if name in user_model:\n",
        "    print(\"ChatPoet: \" + name + \"! A pleasure to see you once more. Start talking or press X to exit.\")\n",
        "  else: \n",
        "    user_model[name] = []\n",
        "    print(\"ChatPoet: It is wonderful to meet you \" + name + \". Start talking or press X to exit.\")\n",
        "\n",
        "  # start conversation\n",
        "  chat = True\n",
        "\n",
        "  while chat:\n",
        "    user_said = input(\"You: \")\n",
        "\n",
        "    if user_said == \"X\":\n",
        "      chat = False\n",
        "      print(\"ChatPoet: I bid you adieu.\")\n",
        "    else:\n",
        "      # hardcoded responses for personal info\n",
        "      for i in info_phrases:\n",
        "        if i in user_said:\n",
        "          # save in user model\n",
        "          user_model[name].append(user_said)\n",
        "\n",
        "          # add personalized response for certain phrases\n",
        "          if i == \"I am\" or i == \"I'm\":\n",
        "            # clean input to reframe\n",
        "            modify_input = user_said.replace(i, \"\")\n",
        "            modify_input = modify_input.rstrip()\n",
        "            if modify_input[-1] in string.punctuation:\n",
        "              modify_input = modify_input[:-1]\n",
        "            \n",
        "            # perform sentiment analysis\n",
        "            if is_positive(user_said) > 0:\n",
        "              print(\"ChatPoet: You are\" + modify_input + \"? Lovely to hear!\")\n",
        "            else: \n",
        "              print(\"ChatPoet: You are\" + modify_input + \"? I'm sorry to hear.\")\n",
        "\n",
        "      # generate response\n",
        "      response = generate_reply(user_said, terms, stanzas, kb)\n",
        "      if response != None:\n",
        "        print(\"ChatPoet: \" + response)\n",
        "      else: \n",
        "        print(\"ChatPoet: Sorry, I didn't quite understand. Let us converse on a different topic.\")\n",
        "        if name in user_model:\n",
        "          mention = random.choice(user_model[name])\n",
        "          print(\"ChatPoet: Earlier you had told me /'\" + mention + \"/', how do you feel now?\")\n",
        "\n",
        "  with open('user_model.pkl', 'wb') as f:\n",
        "    pickle.dump(user_model, f)\n",
        "\n",
        "  # for key,value in user_model.items():\n",
        "  #   print(key, ':', value)\n",
        "\n",
        "# start\n",
        "run_chatbot()"
      ],
      "metadata": {
        "id": "zTO5mn8MDNJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}